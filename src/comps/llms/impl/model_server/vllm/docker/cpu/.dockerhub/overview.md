# OPEA ERAG vLLM Model Server for IntelÂ® XeonÂ® Processor

Part of the IntelÂ® AI for Enterprise RAG (ERAG) ecosystem.

## ğŸ” Overview

This vLLM Model Server enables LLM inference on CPU platforms using the [vLLM](https://github.com/vllm-project/vllm). Designed to run on IntelÂ® XeonÂ® processors, it provides a robust serving endpoint ideal for CPU-centric environments or those with limited or unavailable GPU or HPU resources.

### Features

- Enables LLM serving relying on CPUs
- Utilizes IntelÂ® XeonÂ® Processors features like AMX (Advanced Matrix Extensions) and AVX-512 for enhanced performance

## ğŸ”— Related Components

This service integrates with OPEA ERAG LLM Microservice.

## License

OPEA ERAG is licensed under the Apache License, Version 2.0.

Copyright Â© 2024â€“2025 Intel Corporation. All rights reserved.