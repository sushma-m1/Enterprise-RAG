# OPEA ERAG vLLM Model Server for IntelÂ® GaudiÂ® AI Accelerators

Part of the IntelÂ® AI for Enterprise RAG (ERAG) ecosystem.

## ğŸ” Overview

This vLLM Model Server delivers high-performance Large Language Model inference on IntelÂ® GaudiÂ® AI Accelerators using the tailored vLLM backend, see [HabanaAI/vllm-fork](https://github.com/HabanaAI/vllm-fork.git). 

### Features

- Optimized for IntelÂ® GaudiÂ® AI Accelerators with full hardware acceleration support
- Advanced features including FP8 quantization for improved performance

## ğŸ”— Related Components

This service integrates with OPEA ERAG LLM Microservice.

## License

OPEA ERAG is licensed under the Apache License, Version 2.0.

Copyright Â© 2024â€“2025 Intel Corporation. All rights reserved.