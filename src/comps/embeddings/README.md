# Embeddings Microservice

The Embedding Microservice is designed to efficiently convert textual strings into vectorized embeddings, facilitating seamless integration into various machine learning and data processing workflows. This service utilizes advanced algorithms to generate high-quality embeddings that capture the semantic essence of the input text, making it ideal for applications in natural language processing, information retrieval, and similar fields.

**Key Features**

- **High Performance**: Optimized for quick and reliable conversion of textual data into vector embeddings.
- **Scalability**: Built to handle high volumes of requests simultaneously, ensuring robust performance even under heavy loads.
- **Ease of Integration**: Provides a simple and intuitive API, allowing for straightforward integration into existing systems and workflows.
- **Customizable**: Supports configuration and customization to meet specific use case requirements, including different embedding models and preprocessing techniques.

Users are able to configure and build embedding-related services according to their actual needs.

## Support matrix

Support for specific model servers with Dockerfiles or build instruction.

| Model server                | langchain | llama_index |
| ------------                | ----------| ------------|
| [torchserve](./impl/model-server/torchserve)  | &#x2713;  | &#x2717;    |
| [TEI](./impl/model-server/tei/)                | &#x2713;  | &#x2713;    |
| [OVMS](./impl/model-server/ovms)              | &#x2717;  | &#x2717;    |
| [mosec](./impl/model-server/mosec)            | &#x2713;  | &#x2717;    |

## Configuration Options

The configuration for the Embedding Microservice is specified in the [impl/microservice/.env](impl/microservice/.env) file. You can adjust these settings by modifing this dotenv file or by exporting environment variables.

| Environment Variable            | Description                                                                                                           |
|---------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| `EMBEDDING_USVC_PORT`                 | The port of the microservice, by default 6000.                                                                        |
| `EMBEDDING_MODEL_NAME`                | The name of language model to be used (e.g., "bge-large-en-v1.5")                                             |
| `EMBEDDING_CONNECTOR`                 | The framework used to connect to the model. Supported values: 'langchain', 'llama_index' |
| `EMBEDDING_MODEL_SERVER`              | Specifies the type of model server (e.g. "tei", "ovms")                                                               |
| `EMBEDDING_MODEL_SERVER_ENDPOINT`     | URL of the model server endpoint, e.g., "http://localhost:8090"                                                       |


## Getting started

### Prerequisite: Start Embdedding Model Server

The Embedding Microservice interacts with an Embedding model endpoint, which must be operational and accessible at the the URL specified by the `EMBEDDING_MODEL_SERVER_ENDPOINT` env.

Depending on the model server you want to use, follow the approppriate instructions in the [impl/model_server](impl/model_server/) directory to set up and start the service. 


Currently, we provide these ways to implement a model server for an embedding:

1. Build embedding model server based on the [**_TEI endpoint_**](./impl/model-server/tei/), which provides more flexibility, but may bring some network latency.
2. Utilize [**_Torchserve_**](./impl/model-server/torchserve/), which supports [IntelÂ® Extension for PyTorch*](https://github.com/intel/intel-extension-for-pytorch) for a performance boost on Intel-based Hardware.
3. Utilize [**_Mosec_**](./impl/model-server/mosec/) to run a model server with IntelÂ® Extension for PyTorch* optimizations.
4. Run an embedding model server with [**_OVMS_**](./impl/model-server/ovms/) - an open source model server built on top of the OpenVINOâ„¢ toolkit, which enables optimized inference across a wide range of hardware platforms.

Refer to `README.md` of a particular library to get more information on starting a model server.


## ðŸš€1. Start Embedding Microservice with Python (Option 1)

To start the Embedding microservice, you need to install all the dependencies first.

#### 1.1. Install Requirements
To freeze the dependencies of a particular microservice, we utilize [uv](https://github.com/astral-sh/uv) project manager. So before installing the dependencies, installing uv is required.
Next, use `uv sync` to install the dependencies. This command will create a virtual environment.

```bash
pip install uv
uv sync --locked --no-cache --project impl/microservice/pyproject.toml
source impl/microservice/.venv/bin/activate
```

#### 1.2. Start Microservice

```bash
python opea_embedding_microservice.py
```

### ðŸš€2. Start Embedding Microservice with Docker (Option 2)

#### 2.1. Build the Docker Image:
Navigate to the `src` directory and use the docker build command to create the image:
```bash
cd ../../
docker build -t opea/embedding:latest -f comps/embeddings/impl/microservice/Dockerfile .

```
Please note that the building process may take a while to complete.

#### 2.2. Run the Docker Container:

Ensure that the `EMBEDDING_CONNECTOR` corresponds to the specific image built with the relevant requirements. Below are examples for both Langchain and Llama Index:

```bash
# for langchain
docker run -d --name="embedding-microservice" \
  -e EMBEDDING_CONNECTOR=langchain \
  --net=host \
  --ipc=host \
  opea/embedding
```

```bash
# for llama_index
docker run -d --name="embedding-microservice" \
  -e EMBEDDING_CONNECTOR=llama_index \
  --net=host \
  --ipc=host \
  opea/embedding
```

If the model server is running at a different endpoint than the default, update the `EMBEDDING_MODEL_SERVER_ENDPOINT` variable accordingly. Here's an example of how to pass configuration using the docker run command:

```bash
# for langchain
docker run -d --name="embedding-microservice" \
  -e EMBEDDING_MODEL_SERVER_ENDPOINT="http://localhost:8090" \
  -e EMBEDDING_MODEL_NAME="bge-large-en-v1.5" \
  -e EMBEDDING_CONNECTOR="langchain" \
  -e EMBEDDING_MODEL_SERVER="tei" \
  --net=host \
  --ipc=host \
  opea/embedding
```

```bash
# for llama_index
docker run -d --name="embedding-microservice" \
  -e EMBEDDING_MODEL_SERVER_ENDPOINT="http://localhost:8090" \
  -e EMBEDDING_MODEL_NAME="bge-large-en-v1.5" \
  -e EMBEDDING_CONNECTOR="llama_index" \
  -e EMBEDDING_MODEL_SERVER="tei" \
  --net=host \
  --ipc=host \
  opea/embedding
```


### 3. Verify the Embedding Microservice

#### 3.1. Check Status

```bash
curl http://localhost:6000/v1/health_check \
  -X GET \
  -H 'Content-Type: application/json'
```

####  3.2. Sending a Request

The embedding microservice accepts input as either a single text string or multiple documents containing text. Below are examples of how to structure the reques

**Example Input**

 For a single text input:
  ```bash
  curl http://localhost:6000/v1/embeddings \
    -X POST \
    -d '{"text":"Hello, world!"}' \
    -H 'Content-Type: application/json'
  ```

For multiple documents:
```bash
curl http://localhost:6000/v1/embeddings\
  -X POST \
  -d '{"docs": [{"text":"Hello, world!"}, {"text":"Hello, world!"}]}' \
  -H 'Content-Type: application/json'
```

**Example Output**

The output of an embedding microservice is a JSON object that includes the input text, the computed embeddings, and additional parameters.

For a single text input:
```json
{
  "id":"d4e67d3c7353b13c3821d241985705b1",
  "text":"Hello, world!",
  "embedding":[ 0.024471128, 0.047724035, -0.02704641, 0.0013827643 ],
  "search_type":"similarity",
  "k":4,
  "distance_threshold":null,
  "fetch_k":20,
  "lambda_mult":0.5,
  "score_threshold":0.2,
  "metadata": {}
}
```

For multiple documents:
```json
{
  "id":"d4e67d3c7353b13c3821d241985705b1",
  "docs": [
    {
      "id": "27ff622c495813be476c892bb6940bc5",
      "text":"Hello, world!",
      "embedding":[ 0.024471128, 0.047724035, -0.02704641, 0.0013827643 ],
      "search_type":"similarity",
      "k":4,
      "distance_threshold":null,
      "fetch_k":20,
      "lambda_mult":0.5,
      "score_threshold":0.2
    },
    {
      "id": "937f9b71a2fa0e6437e33c55bec8e1ea",
      "text": "Hello, world!",
      "embedding":[ 0.024471128, 0.047724035, -0.02704641, 0.0013827643 ],
      "search_type":"similarity",
      "k":4,
      "distance_threshold":null,
      "fetch_k":20,
      "lambda_mult":0.5,
      "score_threshold":0.2,
      "metadata": {}
    }
  ]
}
```


## Additional Information
### Project Structure

The project is organized into several directories:
- `impl/`: This directory contains the implementation. It includes the microservice folder with the Dockerfile for the microservice, and the `model_server` directory, which provides setup and running instructions for various model servers, such as TEI or OVMS.
- `utils/`: This directory contains utility scripts and modules that are used by the Embedding Microservice.

The tree view of the main directories and files:

```bash
  .
  â”œâ”€â”€ impl
  â”‚   â”œâ”€â”€ microservice
  â”‚   â”‚   â”œâ”€â”€ Dockerfile
  â”‚   â”‚   â”œâ”€â”€ pyproject.toml
  â”‚   â”‚   â”œâ”€â”€ uv.lock
  â”‚   â”‚   â””â”€â”€ .env
  â”‚   â”œâ”€â”€ model_server/
  â”‚   â”‚   â”œâ”€â”€ tei/
  â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
  â”‚   â”‚   â”‚   â”œâ”€â”€ run_tei.sh
  â”‚   â”‚   â”‚   â””â”€â”€ docker/
  â”‚   â”‚   â”‚       â”œâ”€â”€ .env
  â”‚   â”‚   â”‚       â””â”€â”€ docker-compose.yml
  â”‚   â”‚   â”‚
  â”‚   â”‚   â””â”€â”€ ...
  â”‚   â””â”€â”€ ...
  â”‚
  â”œâ”€â”€ utils/
  â”‚   â”œâ”€â”€ opea_embedding.py
  â”‚   â”œâ”€â”€ api_config/
  â”‚   â”‚   â””â”€â”€ api_config.yml
  â”‚   â”‚
  â”‚   â””â”€â”€ connectors/
  â”‚       â”œâ”€â”€ connector.py
  â”‚       â”œâ”€â”€ connector_langchain.py
  â”‚       â””â”€â”€ connector_lamaindex.py
  â”‚
  â”œâ”€â”€ README.md
  â””â”€â”€ opea_embedding_microservice.py
```

#### Tests
- `src/tests/unit/embeddings/`: Contains unit tests for the Embedding Microservice components

