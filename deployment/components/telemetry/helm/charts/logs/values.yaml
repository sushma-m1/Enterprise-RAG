# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
#

### =================================================
### opentelemetry-collector for logs subchart
### =================================================
otelcol-logs:

  enabled: true

  ### ----------------------
  # Possible values:
  # git clone https://github.com/open-telemetry/opentelemetry-helm-charts
  # opentelemetry-helm-charts/charts/opentelemetry-collector/values.yaml
  mode: daemonset

  image:
    # Check manifests.yaml for different components available:
    # https://github.com/open-telemetry/opentelemetry-collector-releases/tree/main/distributions
    #repository: "otel/opentelemetry-collector-k8s"
    repository: "otel/opentelemetry-collector-contrib"

  # to access pod otelcol by node ip (only for debug)
  # BREAKS access to internal cluster kubernetes network
  # no access to e.g. loki-gateway.monitoring.svc.cluster.local
  #hostNetwork: true

  # only works for "deployment" mode !!!
  #serviceMonitor:
  #  enabled: true
  #  extraLabels:
  #    release: telemetry

  # only works for "daemonset" mode !!!
  podMonitor:
    enabled: true
    extraLabels: # TODO to be deleted when included inside single rag-telemetry helm-chart
      release: telemetry

  presets:
    logsCollection:
      enabled: true # this enable filelog receiver configured for k8s
      #enabled: false # disable for debugging
      includeCollectorLogs: false # default: false, do not include internal logs, to prvent logs explosion https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector#warning-warning-risk-of-looping-the-exported-logs-back-into-the-receiver-causing-log-explosion
    kubernetesAttributes:
      enabled: true

  resources:
    limits:
      memory: 200Mi

  service:
    type: ClusterIP

  # creates Kuberntes Services
  ports:
    # disable otlp ports (not used yet for tracing push)
    otlp:
      enabled: false
    otlp-http:
      enabled: false
    # disable not used ports
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    zpages:
      enabled: true
      containerPort: 55679
      servicePort: 55679
      hostPort: 55679
      protocol: TCP
    metrics:
      # The metrics port is disabled by default. However you need to enable the port
      # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
      enabled: true
      containerPort: 8888
      servicePort: 8888
      protocol: TCP
    # prometheus:
    #   enabled: true
    #   containerPort: 9464
    #   servicePort: 9464
    #   protocol: TCP


  # Use alternateConfig to be able to null pipelines/receivers: please see check this for details
  # https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/values.yaml#L187
  # config:
  alternateConfig:
    ### ----------------------------
    ### Otelcol configuration
    ### ----------------------------

    #priority: info
    ### Disable unused receivers
    # https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-collector#basic-top-level-configuration
    # This also requires disabling traces/metrics pipeline in services/pipeline.
    receivers:
      #filelog: {} # required: enable only one, will be merged with "helm chart" logsCollection.enabled for Kubernetes pods

      ## NOTE not needed as subchart with alternateConfig!!!
      # traces/spans receivers are disabled we will have additional otelcol for traces
      # jaeger: null
      # zipkin: null
      # prometheus: null
      # otlp: null #

      filelog:
        include:
          - /var/log/pods/*/*/*.log
        # start_at: beginning
        include_file_name: false
        include_file_path: true
        preserve_trailing_whitespaces: true
        preserve_leading_whitespaces: false
        # List all operators
        # https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/README.md
        operators:
        ### First step of log processing. Unwrap from cotainerd format.
        # copied from original logCollection helm template
        - id: container-parser
          max_log_size: 102400
          type: container
          format: containerd
          output: router

        ### Second step. Routing based on container name. After that severity parsing, next step is to the last one noop, to avoid go throught all parsers.
        # How control flow https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/types/operators.md
        # How set router https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/pkg/stanza/docs/operators/router.md
        # And language to express conditions https://github.com/expr-lang/expr/blob/master/docs/language-definition.md
        - type: router
          id: router
          routes:
            - expr: 'resource["k8s.container.name"] matches "^embedding-usvc$"'
              output: embedding-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^edp-ingestion$"'
              output: embedding-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^torchserve-embedding$"'
              output: torchserve-severity-parser-a
            - expr: 'resource["k8s.container.name"] matches "^torchserve-reranking$"'
              output: torchserve-severity-parser-a
            - expr: 'resource["k8s.container.name"] matches "^router-server$"'
              output: router-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^edp-hierarchical-dataprep$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^edp-text-extractor$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^edp-text-compression$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^edp-text-splitter$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^llm-usvc$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^prmpt-tmpl-usvc$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^reranking-usvc$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^retriever-usvc$"'
              output: python-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^teirerank$"'
              output: tei-tgi-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^tgi$"'
              output: tei-tgi-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^tempo$"'
              output: loki-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^loki$"'
              output: loki-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^nginx$"'
              output: loki-severity-parser-http-code
            - expr: 'resource["k8s.container.name"] matches "^loki-canary$"'
              output: canary-severity-parser-a
            - expr: 'resource["k8s.container.name"] matches "^kindnet-cni$"'
              output: kube-system_kindnet-severity-parser
            - expr: 'resource["k8s.container.name"] matches "^mongodb$"'
              output: mongo-severity-parser
          default: noop

        #### EMBEDDING
        - id: embedding-severity-parser
          type: regex_parser
          on_error: "send_quiet"
          parse_from: body
          output: noop
          regex: >
            '^(?P<severity_field>.*):\s+(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(?P<port>\d+)\s+-\s+"(?P<method>GET|POST|HEAD|PUT|DELETE|PATCH|OPTIONS)\s+(?P<path>[^ ]+)\s+HTTP\/(?P<http_version>\d\.\d)"\s+(?P<status_code_http>\d{3})\s+(?P<status_text_http>[^ ]+)'
          severity:
            parse_from: attributes.severity_field
            overwrite_text: true
            mapping:
              info: INFO
              warn: WARN
              error: ERROR
              debug: DEBUG
        #### EMBEDDING

        #### TORCHSERVE
        - id: torchserve-severity-parser-a
          type: regex_parser
          on_error: "send_quiet"
          parse_from: body
          output: torchserve-severity-parser-b
          regex: >
            '(?P<log_timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2},\d+)\s+\[(?P<severity_field>[A-Z]+)\s*\]\s+((?P<thread>[^ ]+)\s+(?P<log_type>[^ ]+)\s+-\s+(?P<ip>[^ ]+):(?P<port>\d+)\s+"(?P<method>GET|POST|HEAD|PUT|DELETE|PATCH|OPTIONS)\s+(?P<path>[^ ]+)\s+HTTP\/(?P<http_version>\d\.\d)"\s+(?P<status_code_http>\d{3})\s+(?P<response_time>\d+))$'
          severity:
            parse_from: attributes.severity_field
            overwrite_text: true
            mapping:
              info: INFO
              warn: WARN
              error: ERROR
              debug: DEBUG

        - id: torchserve-severity-parser-b
          type: regex_parser
          on_error: "send_quiet"
          parse_from: body
          if: attributes.severity_field == nil
          output: noop
          regex: '(?P<log_timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2},\d+)\s+\[(?P<severity_field>[A-Z]+)\s*\]\s*(?P<message>.*)$'
          severity:
            parse_from: attributes.severity_field
            overwrite_text: true
            mapping:
              info: INFO
              warn: WARN
              error: ERROR
              debug: DEBUG
        #### TORCHSERVE

        #### ROUTER
        - id: router-severity-parser
          type: json_parser
          on_error: "send_quiet"
          parse_from: body
          output: noop
          severity:
            parse_from: attributes.level
            overwrite_text: true
        #### ROUTER

        #### LLM RERANKING RETRIEVER TEXT_EXTRACTOR TEXT_COMPRESSION TEXT_SPLITTER
        - id: python-severity-parser
          type: regex_parser
          on_error: "send_quiet"
          parse_from: body
          output: noop
          regex: >
            '^(?P<severity_field>.*):\s+(?P<ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(?P<port>\d+)\s+-\s+"(?P<method>GET|POST|HEAD|PUT|DELETE|PATCH|OPTIONS)\s+(?P<path>[^ ]+)\s+HTTP\/(?P<http_version>\d\.\d)"\s+(?P<status_code>\d{3})\s+(?P<status_text>[^ ]+)'
          severity:
            parse_from: attributes.severity_field
            overwrite_text: true
            mapping:
              info: INFO
              warn: WARN
              error: ERROR
              debug: DEBUG
        #### LLM RERANKING RETRIEVER TEXT_EXTRACTOR TEXT_COMPRESSION TEXT_SPLITTER

        #### TEI TGI
        - id: tei-tgi-severity-parser
          type: regex_parser
          #on_error: "send_quiet"
          parse_from: body
          output: noop
          # more complex regex not working because log message contains extra ascii formating
          #regex: '^(?P<timestampapp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+Z)\s+(?P<level>INFO|WARN|ERROR|DEBUG)\s+(?:(?P<details>.*)\s)?(?P<logger>[a-zA-Z_:]*:)\s+(?P<file>[^:]+):(?P<line>\d+):\s+(?P<message>.+)$'
          regex: '.*(?P<level>INFO|WARN|ERROR|DEBUG)(?P<text>.*)'
          severity:
            parse_from: attributes.level
            overwrite_text: true
            mapping:
              info: INFO
              warn: WARN
              error: ERROR
              debug: DEBUG
        #### TEI TGI

        #### LOKI
        - id: loki-severity-parser
          type: key_value_parser
          on_error: "send_quiet"
          parse_from: body
          output: noop
          severity:
            parse_from: attributes.level
            overwrite_text: true

        - id: loki-severity-parser-http-code
          type: regex_parser
          on_error: "send_quiet"
          parse_from: body
          output: noop
          regex: '(?P<status_code>\d{3}) "(?P<method>GET|POST|HEAD|PUT|DELETE|PATCH|OPTIONS)'
          severity:
            parse_from: attributes.status_code
            overwrite_text: true
            mapping:
              info:
                - 1xx
                - 101
                - 2xx
              debug:
                - 3xx
              warn:
                - 4xx
              error:
                - 5xx
        #### LOKI

        #### CANARY
        - id: canary-severity-parser-a
          type: add
          output: canary-severity-parser-b
          field: attributes.severity_text
          value: DEBUG

        - id: canary-severity-parser-b
          type: add
          output: canary-severity-parser-c
          field: attributes.severity_number
          value: 5

        - id: canary-severity-parser-c
          type: severity_parser
          output: noop
          parse_from: attributes.severity_text
          overwrite_text: true
          mapping:
            debug: DEBUG
        #### CANARY

        #### KINDNET
        - id: kube-system_kindnet-severity-parser
          type: regex_parser
          on_error: "send_quiet"
          parse_from: body
          output: noop
          regex: '^(?P<severity_field>[IWEFD])(?P<code>\d{4})\s+(?P<time>\d{2}:\d{2}:\d{2}\.\d+)\s+(?P<process_id>\d+)\s+(?P<file>[^:]+):(?P<line>\d+)\]\s+(?P<message>.+)$'
          severity:
            parse_from: attributes.severity_field
            overwrite_text: true
            mapping:
              info: I
              warn: W
              error: E
              debug: D
              fatal: F
        #### KINDNET

        #### MONGO
        - id: mongo-severity-parser
          type: json_parser
          on_error: "send_quiet"
          parse_from: body
          output: noop
          severity:
            parse_from: attributes.s
            overwrite_text: true
            mapping:
              info: I
              warn: W
              error: E
              debug: D
              fatal: F
        #### MONGO

        # The noop operator makes no changes to a entry
        - type: noop

    exporters:
      ## Loki native OTLP protocol
      # https://grafana.com/docs/loki/latest/send-data/otel/
      # https://grafana.com/docs/grafana-cloud/send-data/otlp/send-data-otlp/
      # https://grafana.com/docs/loki/latest/send-data/otel/native_otlp_vs_loki_exporter/
      # Requires: limits_config: allow_structured_metadata: true in Loki configuration
      otlphttp/loki:
        ### monolithic
        # with direct connection (specify both port)
        endpoint: http://telemetry-logs-loki.monitoring.svc.cluster.local:3100/otlp/
        # with gateway
        #endpoint: http://loki-gateway.monitoring.svc.cluster.local/otlp/
        ### simple scalable using service loki-write
        #endpoint: http://loki-write.monitoring.svc.cluster.local:3100/otlp/
        auth:
          authenticator: headers_setter/orgentrag

      ## Debug exporter
      debug: {}
        #use_internal_logger: true
        #verbosity: basic
        #verbosity: normal
        # WARNING: very verbose every message is shown with metadata
        #verbosity: detailed

    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
      headers_setter/orgentrag:
        headers:
          - action: insert
            key: X-Scope-OrgID
            #from_context: tenant_id
            value: entrag
      zpages:
        endpoint: ${env:MY_POD_IP}:55679

    service:
      extensions:
        - health_check
        - zpages
        - headers_setter/orgentrag
      telemetry:
        logs:
          # the same can be achieved by adding pod.container.args: --set=service::telemetry::logs::level=debug
          level: info
          #level: debug
      pipelines:
        ### Disable unused pipelines
        ## https://github.com/open-telemetry/opentelemetry-helm-charts/blob/main/charts/opentelemetry-collector/README.md#basic-top-level-configuration
        # traces:
          # receivers: ['otlp']
          # exporters: ['otlp']
        # metrics: null
        logs:
          receivers:
          - filelog
          # pipeline for metrics send by application !!! from otel-demo otlp protocol
          # - otlp
          exporters:
          - otlphttp/loki
          # - debug

### =================================================
### LOKI subchart configuraion
### =================================================
loki:
  enabled: true

  ### ----------------------
  # https://github.com/grafana/loki/issues/7287#issuecomment-1282339134
  # DNS service need to match deployed kubernetes
  # kubectl get svc --namespace=kube-system -l k8s-app=kube-dns  -o jsonpath='{.items..metadata.name}'
  # or disable with extra helm argument: "--set loki.global.dnsService=null" when install telemetry-logs
  global:
    #dnsService: "kube-dns"   # for kind based setup - WARNING: this cannot be null/nil/""/''/ / because will generate broken resolver value ".kube-system.svc.cluster.local." see https://github.com/helm/helm/pull/12879 or Jira IEASG-166 for more details
    dnsService: "coredns"     # for kubespray based setup

  ### ----------------------------
  ### Loki Deployment model
  ### ----------------------------
  # To simplify deployment and streamline Istio integration
  deploymentMode: SingleBinary
  ### Single binary mode: ON
  singleBinary:
    replicas: 1
  ### Simple scallable mode OFF
  # keep those values in-sync with commonConfig.replication_factor
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0
  ### Distributed/Microservices mode OFF
  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

  ######  # Shared storage is required for scaling
  ## https://grafana.com/docs/loki/latest/setup/install/helm/configure-storage/#configure-storage
  ## "The scalable installation requires a managed object store such as...  a self-hosted store such as Minio.
  ## The single binary installation can only use the filesystem for storage." ?!
  ##
  minio:
    enabled: true

  #gateway:
  #  ingress:
  #    enabled: true
  #    hosts:
  #      - host: FIXME
  #        paths:
  #          - path: /
  #            pathType: Prefix

  ### ----------------------------
  ### Loki self monitoring
  ### ----------------------------
  # Warning: THIS Is depreacted will be removed in future versions
  monitoring:
    dashboards:
      enabled: false # TODO: most of dashboards are empty - probably because monolithic version
      namespace: monitoring
    rules:
      enabled: true
      labels:
        release: telemetry
    serviceMonitor:
      enabled: true
      labels:
        release: telemetry


    # use the same tenat so loki canary can actually use our pipeline for full tests
    selfMonitoring:
      tenant:
        name: entrag

  gateway:
    # -- Specifies whether the gateway should be enabled
    enabled: true
    service:
      labels:
        "prometheus.io/service-monitor": "false"  # This labels tells LokiService monitor to drop this target, to not fire alert

  lokiCanary:
    # can be tested with:
    # helm test -n monitoring telemetry-logs
    # Requires gateway - only enable if gateway is enabled
    enabled: true
    # 1) we want to test full logs pipeline through OpenTelemetry collector (not just loki)
    push: false
  test:
    # 2) we want to make sure those metrics from canary are available in Prometheus
    # -- Address of the prometheus server to query for the test. This overrides any value set for canaryServiceAddress.
    # This is kept for backward compatibility and may be removed in future releases. Previous value was 'http://prometheus:9090'
    prometheusAddress: "http://telemetry-kube-prometheus-prometheus.monitoring:9090"

  clusterLabelOverride: entrag

  ### ----------------------------
  ### Loki service configuration
  ### ----------------------------
  loki:

    ### HA/replication
    # Version for single node deployment (required for single node setups).
    # **Warning** this disables HA for write path.
    # Note: This need be with sync with number of ingesters (write.replicas) for scalable mode ora (singleBinary.replicase) for monolithic
    commonConfig:
      replication_factor: 1

    # Required for "Logs explorer" Grafana plugin
    pattern_ingester:
      enabled: true


    ### Other:
    # based on https://grafana.com/docs/loki/latest/setup/install/helm/install-monolithic/
    ingester:
      chunk_encoding: snappy
    querier:
      # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
      max_concurrent: 4
    schemaConfig:
      configs:
        - from: 2024-09-01
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
    ruler:
      enable_api: true

    # based on https://grafana.com/docs/loki/latest/setup/install/helm/install-monolithic/
    limits_config:
      # required to allow ingesting metrics from opentelemetry collector in native way
      allow_structured_metadata: true
      # Enables API for quering volume of data (chart/panel with number of entries per minute in logs explorer)
      volume_enabled: true
      ### Note: Disabled because Note: logs should not be thrown away and modified for audit purposes
      # retention_period: 672h # 28 days retention

    ### Note: disabled because logs should not be thrown away and modified for audit purposes (also check limits_config.retention_period
    # compactor:
    #   retention_enabled: true
    #   delete_request_store: s3

    ### Extra indexed dimensions for better Logs explorer "label browser"
    # https://grafana.com/docs/loki/latest/send-data/otel/#format-considerations
    distributor:
      otlp_config:
        default_resource_attributes_as_index_labels:
        - "k8s.node.name" # extra attribute thanks to resourcedetection/k8snode (check values-journalctl.yaml)
        - "cloud.availability_zone"
        - "cloud.region"
        - "container.name"
        - "deployment.environment"
        - "k8s.cluster.name"
        - "k8s.container.name"
        - "k8s.cronjob.name"
        - "k8s.daemonset.name"
        - "k8s.deployment.name"
        - "k8s.job.name"
        - "k8s.namespace.name"
        - "k8s.pod.name"
        - "k8s.replicaset.name"
        - "k8s.statefulset.name"
        - "service.instance.id"
        - "service.name"
        - "service.namespace"
        # keep that with sync with
        # values-journalctl.yaml directive: "processors.transform.log_statements.statements"
        - "journald.transport"
        - "journald.unit"
        - "journald.cmd"
        - "journald.exe"
        - "journald.cmdline"
        - "journald.hostname"

    ### Tracing
    # tracing:
    #   enabled: true
