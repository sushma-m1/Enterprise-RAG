# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Uses Kubespray to deploy Kubernetes cluster and install required components
deploy_k8s: false

# Available options:
# - "local-path-provisioner": Use for single-node deployment. Local path provisioner works only with Kubespray deployment, so deploy_k8s needs to be true to install it
# - "nfs": Use for multi-node deployment or single node with velero backup functionality; can be installed on existing K8s cluster using infrastructure playbook
# - "netapp-trident": Use for NetApp ONTAP storage with Trident CSI driver; can be installed on existing K8s cluster using infrastructure playbook
install_csi: "local-path-provisioner"

# please use variables local_registry and insecure_registry together
# set to true to install registry on K8s , set insecure registry to be able to push and pull images from registry on defined node and defined port.
local_registry: false
#insecure_registry: "<node-name>:32000"

# Setup when install_csi is "nfs"
# Setup NFS server when working on a multi-node cluster which does not have a StorageClass with RWX capability.
# Setting nfs_server_enabled to true will install NFS server together with CSI driver
# and set nfs_csi_storage_class as the default one.
nfs_node_name: "master-1"             # K8s node name on which to set up NFS server
nfs_host_path: "/opt/nfs-data"       # Host path on the K8s node for NFS server
nfs_csi_driver_version: "4.11.0"
nfs_csi_storage_class: "nfs-csi"

# Setup when install_csi is "netapp-trident"
# Configure NetApp Trident CSI driver for ONTAP storage
# Requires existing NetApp ONTAP system with configured SVM
trident_operator_version: "2506.0"    # Trident operator version (becomes 100.2506.0 in Helm chart)
trident_namespace: "trident"          # Kubernetes namespace for Trident
trident_storage_class: "netapp-trident" # StorageClass name for Trident
trident_backend_name: "ontap-nas"     # Backend configuration name
# ONTAP Backend Configuration - FILL THESE VALUES
ontap_management_lif: ""              # ONTAP management LIF IP address
ontap_data_lif: ""                    # ONTAP data LIF IP address  
ontap_svm: ""                         # Storage Virtual Machine (SVM) name
ontap_username: ""                    # ONTAP username with admin privileges
ontap_password: ""                    # ONTAP password
ontap_aggregate: ""                   # ONTAP aggregate name for volume creation

huggingToken: FILL_HERE # Provide your Hugging Face token here

# Provide absolute path to kubeconfig (e.g. /home/ubuntu/.kube/config).
# If you are installing K8s cluster in simplified deployment, your kubeconfig will be created in path "<repository path>/deployment/inventory/test-cluster/artifacts/admin.conf"
kubeconfig: FILL_HERE  # Provide absolute path to kubeconfig (e.g. /home/ubuntu/.kube/config)

httpProxy:
httpsProxy:
# If HTTP/HTTPS proxy is set, update the noProxy field with the following:
noProxy: #"localhost,.svc,.monitoring,.monitoring-traces"

FQDN: "erag.com" # Provide the FQDN for the deployment

gaudi_operator: false # set to true when Gaudi operator is to be installed
habana_driver_version: "1.21.3-57" # habana operator from https://vault.habana.ai/ui/native/habana-ai-operator/driver/

# If you want to use certificates, set autoGenerated to false and provide the paths to the cert and key
certs:
  autoGenerated: true # Generate self-signed certs
  pathToCert: "" # Provide absolute path to cert
  pathToKey: "" # Provide absolute path to key

# For cluster-level services to use prepared certificates, set autoGenerated to false and provide the paths to the cert and key
clusterCerts:
  autoGenerated: true # Generate self-signed certs
  pathToCert: "" # Provide absolute path to cert
  pathToKey: "" # Provide absolute path to key

registry: "docker.io/opea" # alternatively "localhost:5000/erag" for local registry
tag: "1.4.0"
setup_registry: true
use_alternate_tagging: false
helm_timeout: "10m0s"

hpaEnabled: true
enforcePSS: false
tdxEnabled: false

llm_model: "casperhansen/llama-3-8b-instruct-awq"
llm_model_gaudi: "mistralai/Mixtral-8x7B-Instruct-v0.1"
embedding_model_name: "BAAI/bge-base-en-v1.5"
reranking_model_name: "BAAI/bge-reranker-base"

# Topology-aware resource scheduling and CPU pinning for vLLM
# For detailed documentation, refer to: deployment/components/nri-plugin/README.md
balloons:
  enabled: true
  namespace: kube-system # alternatively, set custom namespace for balloons
  services:
    vllm:
      resources:
        requests:
          cpu: 32
          memory: 64Gi
        limits:
          cpu: 32
          memory: 100Gi

pipelines:
  - namespace: chatqa
    samplePath: chatqa/reference-cpu.yaml
    resourcesPath: chatqa/resources-reference-cpu.yaml
    modelConfigPath: chatqa/resources-model-cpu.yaml
    type: chatqa

gmc:
  enabled: true
  namespace: system
  pvc:
    accessMode: ReadWriteOnce
    models:
      modelLlm:
        name: model-volume-llm
        storage: 100Gi
      modelEmbedding:
        name: model-volume-embedding
        storage: 20Gi
      modelReranker:
        name: model-volume-reranker
        storage: 10Gi

ingress:
  enabled: true
  service_type: NodePort # Set it accordingly to environment if Loadbalancer is supported
  namespace: ingress-nginx

keycloak:
  enabled: true
  namespace: auth

apisix:
  enabled: true
  namespace: auth-apisix

istio:
  enabled: true
  namespace: istio-system

velero:
  enabled: false
  namespace: velero

  # name of a velero Backup resource, created in result of backup in velero namespace;
  # if not specified, most recent backup resource will be looked up.
  restore_from: ""

  backup:
    prefix: backup
    restore_prefix: restore
    backupConfig:
      namespaceOrder: [auth, fingerprint, chatqa, chat-history, edp, vdb, system]
      # selectors for resources to backup (or to exclude); multiple selectors will be ANDed
      resourceSelectors:
        - app.kubernetes.io/instance notin (torchserve_embedding,torchserve_reranking,vllm,vllm-gaudi)
      # list of deployments to be cleared in preparation for restore
      deployments:
        auth: [Helm/keycloak,Secret/keycloak,Secret/tls-secret]
        chatqa: [Deployment/embedding-svc-deployment, Deployment/fgp-svc-deployment, Deployment/input-scan-svc-deployment, Deployment/llm-svc-deployment, Deployment/prompt-template-svc-deployment, Deployment/reranking-svc-deployment, Deployment/retriever-svc-deployment, Deployment/router-service-deployment, Secret/mongo-database-secret,Secret/vector-database-config]
        chat-history: [Helm/chat-history,Secret/mongo-database-secret]
        edp: [Helm/edp,Secret/vector-database-config,Secret/tls-secret,Secret/vector-database-config]
        fingerprint: [Helm/fingerprint,Secret/mongo-database-secret]
        vdb: [Helm/vdb,Secret/vector-database-config]
        system: [Helm/gmc]

    # labels to apply to backup object itself
    labels:
      backup-reason: regular
    # labels to apply to a restore object
    restore_labels:
      restore-reason: recovery
    itemOperationTimeout: 2h0m0s
    csiSnapshotTimeout: 2h0m0s
    ttl: 720h

telemetry:
  enabled: true
  monitoring:
    namespace: monitoring
  traces:
    namespace: monitoring-traces

ui:
  enabled: true
  namespace: rag-ui

edp:
  enabled: true
  namespace: edp
  dpGuard:
    enabled: false
  rbac:
    enabled: false
  storageType: minio
  minio:
    bucketNameRegexFilter: ".*"
  s3:
    region: "us-east-1"
    accessKeyId: ""
    secretAccessKey: ""
    sqsEventQueueUrl: ""
    bucketNameRegexFilter: ".*"
  s3compatible:
    region: "us-east-1"
    accessKeyId: ""
    secretAccessKey: ""
    internalUrl: "https://s3.example.com"
    externalUrl: "https://s3.example.com"
    bucketNameRegexFilter: ".*"
  hierarchical_indices:
    enabled: false
    maxNewTokens: 100
    kSummaries: 2
    kChunks: 2
  semantic_chunking:
    enabled: false

fingerprint:
  enabled: true
  namespace: fingerprint

chat_history:
  enabled: true
  namespace: chat-history

# It is recommented to change the default resources for vector database nodes
# For examples, check deployment/components/vector_databases/values.yaml
vector_databases:
  enabled: true
  namespace: vdb
  vector_store: redis-cluster # Supported: redis, redis-cluster
